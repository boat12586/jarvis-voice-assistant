#!/usr/bin/env python3
"""
ğŸ‡¹ğŸ‡­ Advanced Thai Language Processor for JARVIS
à¸£à¸°à¸šà¸šà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸‚à¸±à¹‰à¸™à¸ªà¸¹à¸‡à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰ DeepSeek-R1 à¹à¸¥à¸° mxbai-embed-large
"""

import logging
import re
import json
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import time

# Thai language processing with fallbacks
try:
    import pythainlp
    from pythainlp import word_tokenize, sent_tokenize, pos_tag
    from pythainlp.normalize import normalize
    from pythainlp.transliterate import romanize
    PYTHAINLP_AVAILABLE = True
except ImportError:
    PYTHAINLP_AVAILABLE = False
    logging.warning("PyThaiNLP not available - using basic Thai processing")


@dataclass
class ThaiProcessingResult:
    """à¸œà¸¥à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸ à¸²à¸©à¸²à¹„à¸—à¸¢"""
    original: str
    normalized: str
    tokens: List[str]
    language_detected: str
    confidence: float
    pos_tags: Optional[List[Tuple[str, str]]] = None
    romanized: Optional[str] = None
    cultural_context: Optional[str] = None
    intent: Optional[str] = None
    entities: Optional[List[Dict[str, Any]]] = None


class AdvancedThaiProcessor:
    """à¸£à¸°à¸šà¸šà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸‚à¸±à¹‰à¸™à¸ªà¸¹à¸‡"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.logger = logging.getLogger(__name__)
        self.config = config or {}
        
        # Thai language patterns
        self.thai_patterns = {
            'polite_particles': ['à¸„à¸£à¸±à¸š', 'à¸„à¹ˆà¸°', 'à¸„à¸°', 'à¸ˆà¹‰à¸°', 'à¸™à¸°', 'à¸«à¸£à¸­'],
            'question_words': ['à¸­à¸°à¹„à¸£', 'à¹„à¸«à¸™', 'à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸«à¸£à¹ˆ', 'à¸—à¸³à¹„à¸¡', 'à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£', 'à¹ƒà¸„à¸£', 'à¸à¸µà¹ˆ'],
            'request_patterns': ['à¸Šà¹ˆà¸§à¸¢', 'à¸à¸£à¸¸à¸“à¸²', 'à¹„à¸”à¹‰à¹„à¸«à¸¡', 'à¸«à¸™à¹ˆà¸­à¸¢', 'à¸‚à¸­'],
            'greeting_patterns': ['à¸ªà¸§à¸±à¸ªà¸”à¸µ', 'à¸”à¸µà¸„à¸£à¸±à¸š', 'à¸”à¸µà¸„à¹ˆà¸°', 'à¸«à¸§à¸±à¸”à¸”à¸µ'],
            'farewell_patterns': ['à¸¥à¸²à¸à¹ˆà¸­à¸™', 'à¸šà¸²à¸¢', 'à¹à¸¥à¹‰à¸§à¸à¸šà¸à¸±à¸™à¹ƒà¸«à¸¡à¹ˆ', 'à¹‚à¸Šà¸„à¸”à¸µ']
        }
        
        # Cultural context mapping
        self.cultural_contexts = {
            'formal': ['à¹€à¸£à¸µà¸¢à¸™', 'à¸—à¹ˆà¸²à¸™', 'à¸à¸£à¸²à¸š', 'à¸ªà¸¡à¹€à¸”à¹‡à¸ˆ', 'à¸à¹ˆà¸²à¸šà¸²à¸—'],
            'casual': ['à¹€à¸à¸·à¹ˆà¸­à¸™', 'à¸à¸µà¹ˆ', 'à¸™à¹‰à¸­à¸‡', 'à¹€à¸®à¹‰à¸¢', 'à¸§à¹ˆà¸²à¹„à¸‡'],
            'business': ['à¸šà¸£à¸´à¸©à¸±à¸—', 'à¸›à¸£à¸°à¸Šà¸¸à¸¡', 'à¹‚à¸„à¸£à¸‡à¸à¸²à¸£', 'à¸‡à¸šà¸›à¸£à¸°à¸¡à¸²à¸“', 'à¸œà¸¥à¸‡à¸²à¸™'],
            'technical': ['à¹‚à¸›à¸£à¹à¸à¸£à¸¡', 'à¸„à¸­à¸¡à¸à¸´à¸§à¹€à¸•à¸­à¸£à¹Œ', 'à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µ', 'à¸‹à¸­à¸Ÿà¸•à¹Œà¹à¸§à¸£à¹Œ', 'à¸®à¸²à¸£à¹Œà¸”à¹à¸§à¸£à¹Œ']
        }
        
        # Intent patterns
        self.intent_patterns = {
            'question': ['à¸­à¸°à¹„à¸£', 'à¹„à¸«à¸™', 'à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸«à¸£à¹ˆ', 'à¸—à¸³à¹„à¸¡', 'à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£', '?'],
            'request': ['à¸Šà¹ˆà¸§à¸¢', 'à¸à¸£à¸¸à¸“à¸²', 'à¸‚à¸­', 'à¹„à¸”à¹‰à¹„à¸«à¸¡'],
            'command': ['à¸—à¸³', 'à¹€à¸›à¸´à¸”', 'à¸›à¸´à¸”', 'à¸ªà¹ˆà¸‡', 'à¹à¸ªà¸”à¸‡', 'à¸«à¸²'],
            'greeting': ['à¸ªà¸§à¸±à¸ªà¸”à¸µ', 'à¸”à¸µ', 'à¸«à¸§à¸±à¸”à¸”à¸µ'],
            'information': ['à¸šà¸­à¸', 'à¸­à¸˜à¸´à¸šà¸²à¸¢', 'à¹à¸™à¸°à¸™à¸³', 'à¸‚à¹‰à¸­à¸¡à¸¹à¸¥']
        }
        
        self.logger.info("ğŸ‡¹ğŸ‡­ Advanced Thai Processor initialized")
    
    def process_text(self, text: str) -> ThaiProcessingResult:
        """à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ à¸²à¸©à¸²à¹„à¸—à¸¢"""
        start_time = time.time()
        
        # Detect language
        language, confidence = self._detect_language(text)
        
        # Normalize text
        normalized = self._normalize_thai_text(text)
        
        # Tokenize
        tokens = self._tokenize_thai(normalized)
        
        # POS tagging (if available)
        pos_tags = self._pos_tag_thai(tokens) if PYTHAINLP_AVAILABLE else None
        
        # Romanization (if available)
        romanized = self._romanize_thai(normalized) if PYTHAINLP_AVAILABLE else None
        
        # Cultural context analysis
        cultural_context = self._analyze_cultural_context(text)
        
        # Intent recognition
        intent = self._recognize_intent(text)
        
        # Named entity recognition (basic)
        entities = self._extract_entities(text)
        
        processing_time = time.time() - start_time
        
        self.logger.debug(f"ğŸ‡¹ğŸ‡­ Processed Thai text in {processing_time:.3f}s")
        
        return ThaiProcessingResult(
            original=text,
            normalized=normalized,
            tokens=tokens,
            language_detected=language,
            confidence=confidence,
            pos_tags=pos_tags,
            romanized=romanized,
            cultural_context=cultural_context,
            intent=intent,
            entities=entities
        )
    
    def _detect_language(self, text: str) -> Tuple[str, float]:
        """à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸ à¸²à¸©à¸²"""
        # Count Thai characters
        thai_chars = len(re.findall(r'[à¸-à¹™]', text))
        total_chars = len(re.findall(r'[a-zA-Zà¸-à¹™]', text))
        
        if total_chars == 0:
            return 'unknown', 0.0
        
        thai_ratio = thai_chars / total_chars
        
        if thai_ratio > 0.6:
            return 'th', thai_ratio
        elif thai_ratio > 0.3:
            return 'mixed', thai_ratio
        else:
            return 'en', 1.0 - thai_ratio
    
    def _normalize_thai_text(self, text: str) -> str:
        """à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ à¸²à¸©à¸²à¹„à¸—à¸¢"""
        if PYTHAINLP_AVAILABLE:
            return normalize(text)
        else:
            # Basic normalization
            text = re.sub(r'\s+', ' ', text)  # Multiple spaces
            text = text.strip()
            return text
    
    def _tokenize_thai(self, text: str) -> List[str]:
        """à¹à¸šà¹ˆà¸‡à¸„à¸³à¸ à¸²à¸©à¸²à¹„à¸—à¸¢"""
        if PYTHAINLP_AVAILABLE:
            return word_tokenize(text, engine='longest')
        else:
            # Basic tokenization - split by spaces and punctuation
            return re.findall(r'\S+', text)
    
    def _pos_tag_thai(self, tokens: List[str]) -> List[Tuple[str, str]]:
        """à¸£à¸°à¸šà¸¸à¸Šà¸™à¸´à¸”à¸‚à¸­à¸‡à¸„à¸³"""
        if PYTHAINLP_AVAILABLE:
            return pos_tag(tokens, engine='perceptron')
        else:
            return [(token, 'UNKNOWN') for token in tokens]
    
    def _romanize_thai(self, text: str) -> str:
        """à¹à¸›à¸¥à¸‡à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸›à¹‡à¸™à¸­à¸±à¸à¸©à¸£à¹‚à¸£à¸¡à¸±à¸™"""
        if PYTHAINLP_AVAILABLE:
            return romanize(text, engine='thai2rom')
        else:
            return text
    
    def _analyze_cultural_context(self, text: str) -> str:
        """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸šà¸£à¸´à¸šà¸—à¸—à¸²à¸‡à¸§à¸±à¸’à¸™à¸˜à¸£à¸£à¸¡"""
        text_lower = text.lower()
        
        for context_type, keywords in self.cultural_contexts.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return context_type
        
        # Check politeness level
        polite_count = sum(1 for particle in self.thai_patterns['polite_particles'] 
                          if particle in text_lower)
        
        if polite_count > 0:
            return 'polite'
        
        return 'neutral'
    
    def _recognize_intent(self, text: str) -> str:
        """à¸£à¸°à¸šà¸¸à¸„à¸§à¸²à¸¡à¸•à¸±à¹‰à¸‡à¹ƒà¸ˆ"""
        text_lower = text.lower()
        
        # Check for question patterns
        for pattern in self.thai_patterns['question_words']:
            if pattern in text_lower:
                return 'question'
        
        if '?' in text:
            return 'question'
        
        # Check for request patterns
        for pattern in self.thai_patterns['request_patterns']:
            if pattern in text_lower:
                return 'request'
        
        # Check for greeting patterns
        for pattern in self.thai_patterns['greeting_patterns']:
            if pattern in text_lower:
                return 'greeting'
        
        # Check for specific intents
        for intent_type, patterns in self.intent_patterns.items():
            for pattern in patterns:
                if pattern in text_lower:
                    return intent_type
        
        return 'statement'
    
    def _extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """à¸ªà¸à¸±à¸”à¸«à¸±à¸§à¸‚à¹‰à¸­à¸ªà¸³à¸„à¸±à¸"""
        entities = []
        
        # Time patterns
        time_patterns = [
            (r'(\d{1,2}:\d{2})', 'TIME'),
            (r'(à¸§à¸±à¸™à¸™à¸µà¹‰|à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™|à¸à¸£à¸¸à¹ˆà¸‡à¸™à¸µà¹‰)', 'DATE'),
            (r'(à¸ˆà¸±à¸™à¸—à¸£à¹Œ|à¸­à¸±à¸‡à¸„à¸²à¸£|à¸à¸¸à¸˜|à¸à¸¤à¸«à¸±à¸ªà¸šà¸”à¸µ|à¸¨à¸¸à¸à¸£à¹Œ|à¹€à¸ªà¸²à¸£à¹Œ|à¸­à¸²à¸—à¸´à¸•à¸¢à¹Œ)', 'DAY'),
            (r'(\d+\s*(à¸šà¸²à¸—|à¸”à¸­à¸¥à¸¥à¸²à¸£à¹Œ|à¸¢à¸¹à¹‚à¸£))', 'MONEY'),
            (r'(\d+\s*(à¹€à¸›à¸­à¸£à¹Œà¹€à¸‹à¹‡à¸™à¸•à¹Œ|%))', 'PERCENTAGE')
        ]
        
        for pattern, entity_type in time_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                entities.append({
                    'text': match.group(1),
                    'type': entity_type,
                    'start': match.start(),
                    'end': match.end()
                })
        
        return entities
    
    def generate_thai_response_context(self, user_input: str, intent: str) -> Dict[str, Any]:
        """à¸ªà¸£à¹‰à¸²à¸‡à¸šà¸£à¸´à¸šà¸—à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸•à¸­à¸šà¸à¸¥à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢"""
        result = self.process_text(user_input)
        
        context = {
            'user_language': result.language_detected,
            'cultural_level': result.cultural_context,
            'intent': result.intent,
            'politeness_level': 'formal' if result.cultural_context == 'polite' else 'casual',
            'response_style': 'helpful_thai',
            'should_use_particles': True,
            'recommended_particle': 'à¸„à¸£à¸±à¸š' if 'à¸„à¸£à¸±à¸š' in user_input else 'à¸„à¹ˆà¸°' if 'à¸„à¹ˆà¸°' in user_input else 'à¸„à¸£à¸±à¸š'
        }
        
        # Add conversation suggestions
        if result.intent == 'greeting':
            context['suggested_responses'] = [
                f"à¸ªà¸§à¸±à¸ªà¸”à¸µ{context['recommended_particle']} à¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¹‰à¸œà¸¡à¸Šà¹ˆà¸§à¸¢à¹„à¸«à¸¡{context['recommended_particle']}",
                f"à¸¢à¸´à¸™à¸”à¸µà¸•à¹‰à¸­à¸™à¸£à¸±à¸š{context['recommended_particle']} à¸§à¸±à¸™à¸™à¸µà¹‰à¹€à¸›à¹‡à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸šà¹‰à¸²à¸‡{context['recommended_particle']}"
            ]
        elif result.intent == 'question':
            context['suggested_responses'] = [
                f"à¹ƒà¸«à¹‰à¸œà¸¡à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¹‰{context['recommended_particle']}",
                f"à¸œà¸¡à¸ˆà¸°à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸™à¸µà¹‰à¹ƒà¸«à¹‰{context['recommended_particle']}"
            ]
        
        return context
    
    def format_thai_response(self, response: str, context: Dict[str, Any]) -> str:
        """à¸ˆà¸±à¸”à¸£à¸¹à¸›à¹à¸šà¸šà¸à¸²à¸£à¸•à¸­à¸šà¸à¸¥à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢"""
        if not response:
            return response
        
        # Add politeness particles if needed
        if context.get('should_use_particles', True):
            particle = context.get('recommended_particle', 'à¸„à¸£à¸±à¸š')
            
            # Don't add if already has particle
            if not any(p in response for p in self.thai_patterns['polite_particles']):
                if not response.endswith(('.', '!', '?')):
                    response += particle
                else:
                    response = response[:-1] + particle + response[-1]
        
        return response


def test_thai_processor():
    """à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸šà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸ à¸²à¸©à¸²à¹„à¸—à¸¢"""
    print("ğŸ§ª Testing Advanced Thai Processor...")
    
    processor = AdvancedThaiProcessor()
    
    test_cases = [
        "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š à¸§à¸±à¸™à¸™à¸µà¹‰à¹€à¸›à¹‡à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸šà¹‰à¸²à¸‡à¸„à¸£à¸±à¸š",
        "à¸Šà¹ˆà¸§à¸¢à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š AI à¹ƒà¸«à¹‰à¸«à¸™à¹ˆà¸­à¸¢à¹„à¸”à¹‰à¹„à¸«à¸¡à¸„à¸£à¸±à¸š",
        "à¹€à¸§à¸¥à¸²à¸•à¸­à¸™à¸™à¸µà¹‰à¸à¸µà¹ˆà¹‚à¸¡à¸‡à¹à¸¥à¹‰à¸§?",
        "à¸‚à¸­à¸šà¸„à¸¸à¸“à¸¡à¸²à¸à¸„à¸£à¸±à¸š",
        "Hello, how are you today?"
    ]
    
    for test_text in test_cases:
        print(f"\nğŸ“ Testing: {test_text}")
        result = processor.process_text(test_text)
        
        print(f"   ğŸŒ Language: {result.language_detected} ({result.confidence:.2f})")
        print(f"   ğŸ”¤ Tokens: {result.tokens[:5]}...")  # Show first 5 tokens
        print(f"   ğŸ­ Cultural: {result.cultural_context}")
        print(f"   ğŸ¯ Intent: {result.intent}")
        
        if result.romanized and result.language_detected == 'th':
            print(f"   ğŸ”¤ Romanized: {result.romanized}")
    
    # Test response context generation
    print(f"\nğŸ¤– Testing response context...")
    context = processor.generate_thai_response_context("à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š", "greeting")
    print(f"   Context: {context}")
    
    # Test response formatting
    response = processor.format_thai_response("à¸ªà¸§à¸±à¸ªà¸”à¸µ à¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¹‰à¸Šà¹ˆà¸§à¸¢à¹„à¸«à¸¡", context)
    print(f"   Formatted: {response}")
    
    print("\nâœ… Advanced Thai Processor test completed!")


if __name__ == "__main__":
    test_thai_processor()