#!/usr/bin/env python3
"""
üáπüá≠ Advanced Thai Language Processor for JARVIS
‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ DeepSeek-R1 ‡πÅ‡∏•‡∏∞ mxbai-embed-large
"""

import logging
import re
import json
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import time

# Thai language processing with fallbacks
try:
    import pythainlp
    from pythainlp import word_tokenize, sent_tokenize, pos_tag
    from pythainlp.normalize import normalize
    from pythainlp.transliterate import romanize
    PYTHAINLP_AVAILABLE = True
except ImportError:
    PYTHAINLP_AVAILABLE = False
    logging.warning("PyThaiNLP not available - using basic Thai processing")


@dataclass
class ThaiProcessingResult:
    """‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
    original: str
    normalized: str
    tokens: List[str]
    language_detected: str
    confidence: float
    pos_tags: Optional[List[Tuple[str, str]]] = None
    romanized: Optional[str] = None
    cultural_context: Optional[str] = None
    intent: Optional[str] = None
    entities: Optional[List[Dict[str, Any]]] = None


class AdvancedThaiProcessor:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.logger = logging.getLogger(__name__)
        self.config = config or {}
        
        # Thai language patterns
        self.thai_patterns = {
            'polite_particles': ['‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏Ñ‡πà‡∏∞', '‡∏Ñ‡∏∞', '‡∏à‡πâ‡∏∞', '‡∏ô‡∏∞', '‡∏´‡∏£‡∏≠'],
            'question_words': ['‡∏≠‡∏∞‡πÑ‡∏£', '‡πÑ‡∏´‡∏ô', '‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà', '‡∏ó‡∏≥‡πÑ‡∏°', '‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£', '‡πÉ‡∏Ñ‡∏£', '‡∏Å‡∏µ‡πà'],
            'request_patterns': ['‡∏ä‡πà‡∏ß‡∏¢', '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤', '‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°', '‡∏´‡∏ô‡πà‡∏≠‡∏¢', '‡∏Ç‡∏≠'],
            'greeting_patterns': ['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', '‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏î‡∏µ‡∏Ñ‡πà‡∏∞', '‡∏´‡∏ß‡∏±‡∏î‡∏î‡∏µ'],
            'farewell_patterns': ['‡∏•‡∏≤‡∏Å‡πà‡∏≠‡∏ô', '‡∏ö‡∏≤‡∏¢', '‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏ö‡∏Å‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà', '‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ']
        }
        
        # Cultural context mapping
        self.cultural_contexts = {
            'formal': ['‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏ó‡πà‡∏≤‡∏ô', '‡∏Å‡∏£‡∏≤‡∏ö', '‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à', '‡∏ù‡πà‡∏≤‡∏ö‡∏≤‡∏ó'],
            'casual': ['‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô', '‡∏û‡∏µ‡πà', '‡∏ô‡πâ‡∏≠‡∏á', '‡πÄ‡∏Æ‡πâ‡∏¢', '‡∏ß‡πà‡∏≤‡πÑ‡∏á'],
            'business': ['‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£', '‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì', '‡∏ú‡∏•‡∏á‡∏≤‡∏ô'],
            'technical': ['‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°', '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', '‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ', '‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå', '‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå']
        }
        
        # Intent patterns
        self.intent_patterns = {
            'question': ['‡∏≠‡∏∞‡πÑ‡∏£', '‡πÑ‡∏´‡∏ô', '‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà', '‡∏ó‡∏≥‡πÑ‡∏°', '‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£', '?'],
            'request': ['‡∏ä‡πà‡∏ß‡∏¢', '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤', '‡∏Ç‡∏≠', '‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°'],
            'command': ['‡∏ó‡∏≥', '‡πÄ‡∏õ‡∏¥‡∏î', '‡∏õ‡∏¥‡∏î', '‡∏™‡πà‡∏á', '‡πÅ‡∏™‡∏î‡∏á', '‡∏´‡∏≤'],
            'greeting': ['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', '‡∏î‡∏µ', '‡∏´‡∏ß‡∏±‡∏î‡∏î‡∏µ'],
            'information': ['‡∏ö‡∏≠‡∏Å', '‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢', '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•']
        }
        
        self.logger.info("üáπüá≠ Advanced Thai Processor initialized")
    
    def process_text(self, text: str) -> ThaiProcessingResult:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        start_time = time.time()
        
        # Detect language
        language, confidence = self._detect_language(text)
        
        # Normalize text
        normalized = self._normalize_thai_text(text)
        
        # Tokenize
        tokens = self._tokenize_thai(normalized)
        
        # POS tagging (if available)
        pos_tags = self._pos_tag_thai(tokens) if PYTHAINLP_AVAILABLE else None
        
        # Romanization (if available)
        romanized = self._romanize_thai(normalized) if PYTHAINLP_AVAILABLE else None
        
        # Cultural context analysis
        cultural_context = self._analyze_cultural_context(text)
        
        # Intent recognition
        intent = self._recognize_intent(text)
        
        # Named entity recognition (basic)
        entities = self._extract_entities(text)
        
        processing_time = time.time() - start_time
        
        self.logger.debug(f"üáπüá≠ Processed Thai text in {processing_time:.3f}s")
        
        return ThaiProcessingResult(
            original=text,
            normalized=normalized,
            tokens=tokens,
            language_detected=language,
            confidence=confidence,
            pos_tags=pos_tags,
            romanized=romanized,
            cultural_context=cultural_context,
            intent=intent,
            entities=entities
        )
    
    def _detect_language(self, text: str) -> Tuple[str, float]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤"""
        # Count Thai characters
        thai_chars = len(re.findall(r'[‡∏Å-‡πô]', text))
        total_chars = len(re.findall(r'[a-zA-Z‡∏Å-‡πô]', text))
        
        if total_chars == 0:
            return 'unknown', 0.0
        
        thai_ratio = thai_chars / total_chars
        
        if thai_ratio > 0.6:
            return 'th', thai_ratio
        elif thai_ratio > 0.3:
            return 'mixed', thai_ratio
        else:
            return 'en', 1.0 - thai_ratio
    
    def _normalize_thai_text(self, text: str) -> str:
        """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        if PYTHAINLP_AVAILABLE:
            return normalize(text)
        else:
            # Basic normalization
            text = re.sub(r'\s+', ' ', text)  # Multiple spaces
            text = text.strip()
            return text
    
    def _tokenize_thai(self, text: str) -> List[str]:
        """‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        if PYTHAINLP_AVAILABLE:
            return word_tokenize(text, engine='longest')
        else:
            # Basic tokenization - split by spaces and punctuation
            return re.findall(r'\S+', text)
    
    def _pos_tag_thai(self, tokens: List[str]) -> List[Tuple[str, str]]:
        """‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥"""
        if PYTHAINLP_AVAILABLE:
            return pos_tag(tokens, engine='perceptron')
        else:
            return [(token, 'UNKNOWN') for token in tokens]
    
    def _romanize_thai(self, text: str) -> str:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÇ‡∏£‡∏°‡∏±‡∏ô"""
        if PYTHAINLP_AVAILABLE:
            return romanize(text, engine='thai2rom')
        else:
            return text
    
    def _analyze_cultural_context(self, text: str) -> str:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏≤‡∏á‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°"""
        text_lower = text.lower()
        
        for context_type, keywords in self.cultural_contexts.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return context_type
        
        # Check politeness level
        polite_count = sum(1 for particle in self.thai_patterns['polite_particles'] 
                          if particle in text_lower)
        
        if polite_count > 0:
            return 'polite'
        
        return 'neutral'
    
    def _recognize_intent(self, text: str) -> str:
        """‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à"""
        text_lower = text.lower()
        
        # Check for question patterns
        for pattern in self.thai_patterns['question_words']:
            if pattern in text_lower:
                return 'question'
        
        if '?' in text:
            return 'question'
        
        # Check for request patterns
        for pattern in self.thai_patterns['request_patterns']:
            if pattern in text_lower:
                return 'request'
        
        # Check for greeting patterns
        for pattern in self.thai_patterns['greeting_patterns']:
            if pattern in text_lower:
                return 'greeting'
        
        # Check for specific intents
        for intent_type, patterns in self.intent_patterns.items():
            for pattern in patterns:
                if pattern in text_lower:
                    return intent_type
        
        return 'statement'
    
    def _extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """‡∏™‡∏Å‡∏±‡∏î‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"""
        entities = []
        
        # Time patterns
        time_patterns = [
            (r'(\d{1,2}:\d{2})', 'TIME'),
            (r'(‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ|‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô|‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ)', 'DATE'),
            (r'(‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå|‡∏≠‡∏±‡∏á‡∏Ñ‡∏≤‡∏£|‡∏û‡∏∏‡∏ò|‡∏û‡∏§‡∏´‡∏±‡∏™‡∏ö‡∏î‡∏µ|‡∏®‡∏∏‡∏Å‡∏£‡πå|‡πÄ‡∏™‡∏≤‡∏£‡πå|‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå)', 'DAY'),
            (r'(\d+\s*(‡∏ö‡∏≤‡∏ó|‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå|‡∏¢‡∏π‡πÇ‡∏£))', 'MONEY'),
            (r'(\d+\s*(‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå|%))', 'PERCENTAGE')
        ]
        
        for pattern, entity_type in time_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                entities.append({
                    'text': match.group(1),
                    'type': entity_type,
                    'start': match.start(),
                    'end': match.end()
                })
        
        return entities
    
    def generate_thai_response_context(self, user_input: str, intent: str) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        result = self.process_text(user_input)
        
        context = {
            'user_language': result.language_detected,
            'cultural_level': result.cultural_context,
            'intent': result.intent,
            'politeness_level': 'formal' if result.cultural_context == 'polite' else 'casual',
            'response_style': 'helpful_thai',
            'should_use_particles': True,
            'recommended_particle': '‡∏Ñ‡∏£‡∏±‡∏ö' if '‡∏Ñ‡∏£‡∏±‡∏ö' in user_input else '‡∏Ñ‡πà‡∏∞' if '‡∏Ñ‡πà‡∏∞' in user_input else '‡∏Ñ‡∏£‡∏±‡∏ö'
        }
        
        # Add conversation suggestions
        if result.intent == 'greeting':
            context['suggested_responses'] = [
                f"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ{context['recommended_particle']} ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°{context['recommended_particle']}",
                f"‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö{context['recommended_particle']} ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á{context['recommended_particle']}"
            ]
        elif result.intent == 'question':
            context['suggested_responses'] = [
                f"‡πÉ‡∏´‡πâ‡∏ú‡∏°‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ{context['recommended_particle']}",
                f"‡∏ú‡∏°‡∏à‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ{context['recommended_particle']}"
            ]
        
        return context
    
    def format_thai_response(self, response: str, context: Dict[str, Any]) -> str:
        """‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        if not response:
            return response
        
        # Add politeness particles if needed
        if context.get('should_use_particles', True):
            particle = context.get('recommended_particle', '‡∏Ñ‡∏£‡∏±‡∏ö')
            
            # Don't add if already has particle
            if not any(p in response for p in self.thai_patterns['polite_particles']):
                if not response.endswith(('.', '!', '?')):
                    response += particle
                else:
                    response = response[:-1] + particle + response[-1]
        
        return response


def test_thai_processor():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
    print("üß™ Testing Advanced Thai Processor...")
    
    processor = AdvancedThaiProcessor()
    
    test_cases = [
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡∏ö",
        "‡∏ä‡πà‡∏ß‡∏¢‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AI ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö",
        "‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Å‡∏µ‡πà‡πÇ‡∏°‡∏á‡πÅ‡∏•‡πâ‡∏ß?",
        "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö",
        "Hello, how are you today?"
    ]
    
    for test_text in test_cases:
        print(f"\nüìù Testing: {test_text}")
        result = processor.process_text(test_text)
        
        print(f"   üåê Language: {result.language_detected} ({result.confidence:.2f})")
        print(f"   üî§ Tokens: {result.tokens[:5]}...")  # Show first 5 tokens
        print(f"   üé≠ Cultural: {result.cultural_context}")
        print(f"   üéØ Intent: {result.intent}")
        
        if result.romanized and result.language_detected == 'th':
            print(f"   üî§ Romanized: {result.romanized}")
    
    # Test response context generation
    print(f"\nü§ñ Testing response context...")
    context = processor.generate_thai_response_context("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö", "greeting")
    print(f"   Context: {context}")
    
    # Test response formatting
    response = processor.format_thai_response("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°", context)
    print(f"   Formatted: {response}")
    
    print("\n‚úÖ Advanced Thai Processor test completed!")


if __name__ == "__main__":
    test_thai_processor()